{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.vq import *\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.externals import joblib\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objectDetection(videofile):\n",
    "    cap = cv2.VideoCapture(videofile)\n",
    "    print(\"The video has  {0} frames\".format(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "#     print(\"The video is playing at {} frames per second.\".format(cap.get(cv2.CV_CAP_PROP_FPS)))\n",
    "    # Trained XML classifiers describes some features of some object we want to detect\n",
    "    car_cascade = cv2.CascadeClassifier('cars.xml')\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if ret == False:\n",
    "            break\n",
    "\n",
    "        # Our operations on the frame come here\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detects cars of different sizes in the input image\n",
    "        cars = car_cascade.detectMultiScale(gray, 1.1, 1)\n",
    "\n",
    "        # To draw a rectangle in each cars\n",
    "        for (x,y,w,h) in cars:\n",
    "            cv2.rectangle(gray,(x,y),(x+w,y+h),(0,0,255),2)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('frame',gray)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # When everything done, release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tracking(videofile):\n",
    "    cap = cv2.VideoCapture(videofile)\n",
    "    print('Tracking features for: {}'.format(videofile))\n",
    "    ret, frame1 = cap.read()\n",
    "    frame1 = cv2.resize(frame1, (400, 400)) \n",
    "    prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n",
    "    hsv = np.zeros_like(frame1)\n",
    "    hsv[...,1] = 255\n",
    "    features = []\n",
    "\n",
    "    while(1):\n",
    "        ret, frame2 = cap.read()\n",
    "\n",
    "        if(ret == False):\n",
    "            break\n",
    "\n",
    "        frame2 = cv2.resize(frame2, (400, 400)) \n",
    "        next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\n",
    "#         print(np.array(next).shape)\n",
    "\n",
    "        flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "        mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "        hsv[...,0] = ang*180/np.pi/2\n",
    "        hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "        rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n",
    "        \n",
    "        hof, binedges = np.histogram(ang,bins=450,range=(0,360))\n",
    "\n",
    "        features.append(hof[:8])\n",
    "\n",
    "        cv2.imshow('frame2',rgb)\n",
    "         # Display the resulting frame\n",
    "        cv2.imshow('frame3',next)\n",
    "        k = cv2.waitKey(30) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "       \n",
    "        prvs = next   \n",
    "    \n",
    "#     print('Length of features before sampling {}'.format(len(features)))\n",
    "#     print(len(features))\n",
    "    samplesize = 250\n",
    "    if(len(features) < samplesize):\n",
    "        samplesize = len(features)\n",
    "        \n",
    "    features = random.sample(features, samplesize)\n",
    "#     print(features)\n",
    "#     print('Length of features after sampling {}'.format(len(features)))\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateCodebook(features, k):\n",
    "    print('Generating Code book for all features')\n",
    "    X = np.array(features[0][1])\n",
    "    for path, feature in features[1:]:\n",
    "        X = np.vstack((X, feature))\n",
    "        \n",
    "    print(len(X))\n",
    "    print(X[X == float('nan')])\n",
    "    codebook, variance = kmeans(X.astype(float), k, 1) \n",
    "    return codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateBOW(features, codebook):\n",
    "    print('Create Bag of Visual Words features')\n",
    "    bow = np.zeros((len(features), len(codebook)), \"float32\")\n",
    "    i = 0\n",
    "    for path, feature in features[0:]:\n",
    "        words, d = vq(feature,codebook)\n",
    "        for w in words:\n",
    "            bow[i][w] += 1\n",
    "        i = i + 1\n",
    "    \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractFeatures():\n",
    "    allfeatures = [];\n",
    "    f = range(1,11)\n",
    "    k = 500\n",
    "    classoutput = [];\n",
    "    i = 1;\n",
    "    while(i < 3):\n",
    "        for fn in f:\n",
    "            if(i == 1):\n",
    "                videofile = 'normal/{}.mov'.format(fn)\n",
    "                classoutput.append(1)\n",
    "            if(i == 2):\n",
    "                videofile = 'abnormal/{}.mov'.format(fn)\n",
    "                classoutput.append(-1)\n",
    "                \n",
    "        #     objectDetection(videofile)\n",
    "            feature = tracking(videofile)\n",
    "            allfeatures.append((videofile,feature))\n",
    "        i = i + 1;\n",
    "\n",
    "    codebook = generateCodebook(allfeatures, k)\n",
    "    bagOfWordsFeatures = generateBOW(allfeatures, codebook)\n",
    "        \n",
    "#     print(bagOfWordsFeatures)\n",
    "    return bagOfWordsFeatures,classoutput,codebook\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainVideo(features, classes):\n",
    "    print('Train the features')\n",
    "    clf = LinearSVC()\n",
    "    clf.fit(features, classes)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testVideo(model, videofile, codebook):\n",
    "    print('Test the features')\n",
    "    feature = tracking(videofile)\n",
    "    bagOfWordsFeatures = generateBOW([(videofile,feature)], codebook)\n",
    "    output = model.predict(bagOfWordsFeatures)\n",
    "    if(output == 1):\n",
    "        print('The video has normal flow of traffic')\n",
    "    if(output == -1):\n",
    "        print('The video does not have normal flow of traffic')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "        features,classoutput, codebook = extractFeatures()\n",
    "        joblib.dump(codebook, 'codebook.sav')\n",
    "        model = trainVideo(features, classoutput)\n",
    "        joblib.dump(model, 'model.sav')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking features for: normal/1.mov\n",
      "Tracking features for: normal/2.mov\n",
      "Tracking features for: normal/3.mov\n",
      "Tracking features for: normal/4.mov\n",
      "Tracking features for: normal/5.mov\n",
      "Tracking features for: normal/6.mov\n",
      "Tracking features for: normal/7.mov\n",
      "Tracking features for: normal/8.mov\n",
      "Tracking features for: normal/9.mov\n",
      "Tracking features for: normal/10.mov\n",
      "Tracking features for: abnormal/1.mov\n",
      "Tracking features for: abnormal/2.mov\n",
      "Tracking features for: abnormal/3.mov\n",
      "Tracking features for: abnormal/4.mov\n",
      "Tracking features for: abnormal/5.mov\n",
      "Tracking features for: abnormal/6.mov\n",
      "Tracking features for: abnormal/7.mov\n",
      "Tracking features for: abnormal/8.mov\n",
      "Tracking features for: abnormal/9.mov\n",
      "Tracking features for: abnormal/10.mov\n",
      "Generating Code book for all features\n",
      "4900\n",
      "[]\n",
      "Create Bag of Visual Words features\n",
      "Train the features\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testFromSavedModel(videofile):\n",
    "    model = joblib.load('model.sav')\n",
    "    codebook = joblib.load('codebook.sav')\n",
    "    testVideo(model,videofile, codebook)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the features\n",
      "Tracking features for: test/4.mov\n",
      "Create Bag of Visual Words features\n",
      "The video has normal flow of traffic\n"
     ]
    }
   ],
   "source": [
    "testFromSavedModel('test/4.mov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the features\n",
      "Tracking features for: test/24.mov\n",
      "Create Bag of Visual Words features\n",
      "The video does not have normal flow of traffic\n"
     ]
    }
   ],
   "source": [
    "testFromSavedModel('test/24.mov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking features for: test/23.mov\n",
      "(245, 8)\n"
     ]
    }
   ],
   "source": [
    "ftrs = tracking('test/23.mov')\n",
    "print(np.array(ftrs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video has  251.0 frames\n"
     ]
    }
   ],
   "source": [
    "objectDetection('normal/2.mov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
